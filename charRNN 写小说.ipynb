{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "from config import BATCH_SIZE as batch_size\n",
    "from config import TIME_STEPS as time_steps\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# =============== 读取文本内容 ===============\n",
    "f = codecs.open('./data/xiyouji.txt', 'r', encoding='utf-8')\n",
    "data = f.readlines()\n",
    "# data = ''.join(data)\n",
    "\n",
    "#=============== 简单的预处理 ===============\n",
    "# 替换括号里的内容\n",
    "pattern = re.compile(r'\\(.*?\\)')\n",
    "data = [pattern.sub('', line) for line in data]\n",
    "\n",
    "# 删除\\n, \\r,' '\n",
    "data = [word.replace('.', '。') for word in data]\n",
    "data = [word.replace('\\r', '') for word in data]\n",
    "# data = [word.replace(' ', '') for word in data]\n",
    "\n",
    "# 删除章节名称\n",
    "pattern = re.compile(r'.*?第.*?章.*')\n",
    "def isNotChapterName(text):\n",
    "    if pattern.search(text):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "data = [word for word in data if isNotChapterName(word)]\n",
    "\n",
    "# 省略号 => .\n",
    "data = [line.replace('……', '。') for line in data if len(line) > 1]\n",
    "\n",
    "# ==============判断char是否是乱码===================\n",
    "def is_uchar(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if uchar >= u'\\u4e00' and uchar<=u'\\u9fa5':\n",
    "            return True\n",
    "    \"\"\"判断一个unicode是否是数字\"\"\"\n",
    "    if uchar >= u'\\u0030' and uchar<=u'\\u0039':\n",
    "            return True       \n",
    "    \"\"\"判断一个unicode是否是英文字母\"\"\"\n",
    "    if (uchar >= u'\\u0041' and uchar<=u'\\u005a') or (uchar >= u'\\u0061' and uchar<=u'\\u007a'):\n",
    "            return True\n",
    "    if uchar in ('，','。','：','？','“','”','！','；','、','《','》','——'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 将每行的list合成一个长字符串\n",
    "data = ''.join(data)\n",
    "data = [char for char in data if is_uchar(char)]\n",
    "data = ''.join(data)\n",
    "\n",
    "\n",
    "\n",
    "# ==============生成字典===============\n",
    "vocab = set(data)\n",
    "id2char = {i:c for i,c in enumerate(vocab)}\n",
    "char2id = {c:i for i,c in enumerate(vocab)}\n",
    "# 总数\n",
    "VOCAB_NUM = len(id2char)\n",
    "\n",
    "\n",
    "# =====转换数据为数字格式======\n",
    "numdata = [char2id[char] for char in data]\n",
    "numdata = np.array(numdata)\n",
    "batch_num = len(numdata) // (batch_size * time_steps)\n",
    "# print(numdata.shape)\n",
    "\n",
    "# print('数字数据信息：\\n', numdata[:100])\n",
    "# print('\\n文本数据信息：\\n', ''.join([id2char[i] for i in numdata[:100]]))\n",
    "# print(len(numdata))\n",
    "\n",
    "# ============= 定义 dataloader =============\n",
    "def yield_data( batch_size, time_step, data= numdata):\n",
    "    encoder = OneHotEncoder(categories= [range(VOCAB_NUM)])\n",
    "    start = [i * time_step for i in range(batch_size)]\n",
    "    end = [(i+1) * time_step for i in range(batch_size)]\n",
    "    data_num = batch_size * time_step\n",
    "    batch_num = len(data) // data_num\n",
    "    arr = data[:data_num * batch_num]\n",
    "    arr_y = np.roll(arr,-1)\n",
    "    for n in range(0, len(data), data_num):\n",
    "        x = arr[n: n+data_num].reshape(batch_size, time_step)\n",
    "        y = arr_y[n: n+data_num].reshape(batch_size, time_step)\n",
    "        x = encoder.fit_transform(x.reshape(-1,1)).toarray().reshape(x.shape[0], x.shape[1], -1)\n",
    "        yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the net structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab, \n",
    "                 hidden_size= 256,\n",
    "                 num_layers= 2,\n",
    "                 batch_first= True,\n",
    "                 keep_prob= 0.5\n",
    "                ):\n",
    "        # call the super init method\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        # define property\n",
    "        self.vocab = vocab\n",
    "        self.id2char = dict(enumerate(self.vocab))\n",
    "        self.char2id = {c:i for i,c in enumerate(self.vocab)}\n",
    "        self.vocab_num = len(self.vocab)\n",
    "        self.keep_prob= keep_prob\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # net constructure\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size= self.vocab_num,\n",
    "            hidden_size= hidden_size,\n",
    "            num_layers= 2,\n",
    "            batch_first= True,\n",
    "            dropout= self.keep_prob\n",
    "        )\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(self.keep_prob)\n",
    "        \n",
    "        # linear\n",
    "        self.linear = nn.Linear(\n",
    "            hidden_size,\n",
    "            self.vocab_num\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        out1, hidden = self.lstm(X, hidden)\n",
    "        out2 = self.dropout(out1)\n",
    "        # stack the output of the lstm block\n",
    "        out3 = out2.contiguous().view(-1, self.hidden_size)\n",
    "        out4 = self.linear(out3)\n",
    "        return out4, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "use_gpu = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(2958, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (linear): Linear(in_features=256, out_features=2958, bias=True)\n",
      ")\n",
      "batch [0/100] loss 7.985\n",
      "轩诉诉圣蛾肴房房房房拦呻诉房轩鸿匣蛾匣妄匣妄阳半轩鸿诉猴要八馐嵘匣馐圣钩诉嵘匣匣匣诸诉诉诉篮瑜诸八圣赠乡妄圣鲜秦眼蛾烁紊著僧拇烧妄匣眼诉篮追瑜诉撬妄轩呻瑜钩拇糕\n",
      "batch [10/100] loss 6.941\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [20/100] loss 10.237\n",
      "尊木柱惠检夺成醉靴柱脉止止寸止止尊作止享胎名擎官官同止嫉止根止俊官其馋名止柱裂脓胎变止燃寒柱仓止柱皆柱镶变士顶柱迸柱柱裂琳赐裂柱料柱止怀止学乱其盏柱止柱吐柱贝料\n",
      "batch [30/100] loss 6.928\n",
      "“”，，“，，，，，，，”，，王，，，，王，，“，，，“，，，王，，，，“王“，，，，，，“，，，，，王，，，美，，，“，“，“，，，，，，，“，，，，““，美\n",
      "batch [40/100] loss 6.251\n",
      "，，，，，，，，，，，，，，，，，，，，仙，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [50/100] loss 6.112\n",
      "，，，，，，，说。，”，，，，，说，，，，，，，“，，，，，，，，，说，，。，”，，”，””，，”，”。”，，，，我，，“，，，，，，，“”，，，”，，”，，，\n",
      "batch [60/100] loss 6.390\n",
      "，，，，祖，，，，，，，祖，，，，，祖“。，，，不，，，，，，，不祖，，，祖，，，，，，，不，，祖，，，，，，，不，，，，，，”，，说，祖，祖，祖祖，，不，，，\n",
      "batch [70/100] loss 6.483\n",
      "，，，，，，，，，，，，，，，，，祖，，，，，，，，，，，，师能，，，，，，，，，，，，，，，，师，，，，师，，，，师，，，，，，，，，师，，师，，，，，，，，\n",
      "batch [80/100] loss 6.412\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [90/100] loss 6.414\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [100/100] loss 6.514\n",
      "，，，，，，，，，，，，，，王，，，，，，，，，，，，，王，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [110/100] loss 5.974\n",
      "，，，，，，，，，，，，”，，，，，，，，，，，，，，，，，，，，，空王，，，，，，，，，，，，，，，，，。，，，，，，，，”，，，，，，，，，，，，，。，，，\n",
      "batch [120/100] loss 6.209\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [130/100] loss 6.147\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [140/100] loss 6.859\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [150/100] loss 6.007\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，王，，，，，，，，，，，，，，，，，王王，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [160/100] loss 6.221\n",
      "空，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，打，，，，，，打，，，，，，，，，，，，，，这，，，，，，，，，，，，，，\n",
      "batch [170/100] loss 6.025\n",
      "，，，，，，，，。，，大大，，，，，，大，大，，，大，，，，，，，，，大，，，，，，，，，，，，，大，，，，，，，，，，，，王，，，，王，，，，大，大，，大，大\n",
      "batch [180/100] loss 7.079\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，。，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [190/100] loss 6.057\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，圣，，，，，，圣，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [200/100] loss 5.558\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [210/100] loss 6.993\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [220/100] loss 5.643\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [230/100] loss 5.903\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [240/100] loss 6.463\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [250/100] loss 6.511\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [260/100] loss 6.525\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [270/100] loss 6.201\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [280/100] loss 6.006\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [290/100] loss 5.962\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [300/100] loss 6.106\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [310/100] loss 6.268\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [320/100] loss 5.982\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [330/100] loss 5.399\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [340/100] loss 6.397\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [350/100] loss 6.424\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，萨，，，，，，\n",
      "batch [360/100] loss 6.499\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [370/100] loss 6.486\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [380/100] loss 5.736\n",
      "，僧，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，“，，，，，，，，，，，，，，，，，，唐僧，，，，，，，，，，僧，，，，，，，，，，，，，，，，\n",
      "batch [390/100] loss 5.834\n",
      "，，，，，，，僧，，，，，，，，，，，，，，，，，，，唐，，，，，，，，，，，，，，，，，，，，，，，僧，，，，，，，，，，，，，，，，，，，唐，，，，，，，，\n",
      "batch [400/100] loss 5.491\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，僧，，，，，，，，，，，，，““，，，，，，，，，，，，，，，，，，，“，僧，，，，，，，，僧，，，，，，，，，\n",
      "batch [410/100] loss 5.749\n",
      "，，，，，，唐，，，，，，，，，，，，，，，，，，，，唐唐唐，，，，，，唐，，，，，，，，，，，，，”，，，“，，，，，，，僧僧：“唐，，，，，，，，，，，，，\n",
      "batch [420/100] loss 6.486\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，“，，，，，，，，，，，说，，，，，，，，，，，，，，，，，，，，，，，，，，“，，，，，，，\n",
      "batch [430/100] loss 5.666\n",
      "，，，，，，，，，，，，，，，僧，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，僧僧僧\n",
      "batch [440/100] loss 4.929\n",
      "，，，，，僧，，，，，，：，，，，，，，，，，，，，，，，，，”唐僧，：“，，，，，，，：，，，，，，，，，，，，，，，，，，，，，，，，：“，，，，，，，，，\n",
      "batch [450/100] loss 5.630\n",
      "，，“唐唐，，“，，，，”唐，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，僧，，，，僧，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [460/100] loss 5.395\n",
      "，，，，，“悟，，，僧，，”，，，”，：“，，，，，，，，，，，，，，，，，，，，，，，，，，，，，者，，，，，”唐僧，，，，”“，，，，，，，，，，，，，，，\n",
      "batch [470/100] loss 4.972\n",
      "，“我，，，，，，，，，，，，，：唐者，：”我，，，，，，，，，，，，，，，，，，，，，，，，，，，，：唐者，：“我，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [480/100] loss 5.560\n",
      "，，，悟萨，：“空，，，，，，，，，，，，，，，，，，，，，，，悟，，，，，，，，，，，，，，，，，，，，”，，，，，，，，，，，，，，，，，，，悟悟，空，，，\n",
      "batch [490/100] loss 5.610\n",
      "，，，，，，，，，萨，，，，，，，，，，，，，，，，，，，，，，”菩僧，，，，，，，，，，，，萨，菩萨，，，，，，空，，，，，，，，，，，菩，，，父，，，，，，\n",
      "batch [500/100] loss 5.739\n",
      "”“你，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，僧者，，，，，，，，，，“，空，，，，，，，，，，，，，，，，”，，，”，，，，，，”唐僧，\n",
      "batch [510/100] loss 5.874\n",
      "，，，：“，，，，，，，，，，，”唐僧，，，，，，，，，，，，“者，，，，，，僧，，，，，，，，，，，，，，”，，，，，，，，，，，”者，，，，，，，，，，，，\n",
      "batch [520/100] loss 5.479\n",
      "，，，，，，，，，，，，，，，僧，，，，，，，，”，僧，，，，，，，，：，，：，，，，，，，僧，，僧僧，，，，”僧，，，，僧，，，，，，，，”老僧说：：“你，僧\n",
      "batch [530/100] loss 5.776\n",
      "，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，说，，，，，，，，，，，，，，，，，，，，，，，，，尚，，，，，，，，僧，，，“，，，，，\n",
      "batch [540/100] loss 5.961\n",
      "，，，，，，裟，，，，，裟，，说，，裟，，，，，，：“徒僧，，，，，，，，，，，，，，，，，，，，，，，，，，，，，袈，，，，，，，，，，，，，裟，，僧行，，，\n",
      "batch [550/100] loss 6.734\n",
      "，，，，，，，，，，，，，”，，，，，，，，，，：，，，，，，”，“，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，：，，，，，，，\n",
      "batch [560/100] loss 5.837\n",
      "，，，，，，，，，，，，，，，，，，，，空，，：“，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，尚，，\n",
      "batch [570/100] loss 5.421\n",
      "，，，，，，僧，，，，，，，，，，，，，”者，，，，，，，，，，，萨，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，”者\n",
      "batch [580/100] loss 4.762\n",
      "，萨，”行萨，，空，，，，者，行空，：“，空，，，萨，，，，裟，，”萨，，，，，，，，，，，，，，，，，，，行，，，，，”行者，：“行，，萨，，，，，，，，，，\n",
      "batch [590/100] loss 5.613\n",
      "，，，，，：，”，，，，，，，，”菩，，，，，，，“，僧，空，，，，，，，，，，，，僧，，，，，，，，菩空，，，，，，，，：，，，”者，，，，“菩，，，，，僧，\n",
      "batch [600/100] loss 5.740\n",
      "，，，，，，，，，，，，，，，，，僧，，，，，，，，，，，，，，，，，，，，，，，，，，，，“这，，，，，，，”孙，，，，裟，，，，，，，，这，，”者，：“，，\n",
      "batch [610/100] loss 6.091\n",
      "，，，，，，，，行，，，，，，：“这，，，，，，，”，，，，，，，，，”行，，，，，，，，，者，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [620/100] loss 5.528\n",
      "，，，，，，，：，，，，，，，，”怪，，，：“你，，，，，，说，，，，，，”，，，，，”行者，，，：“你，，，，，，，，，，，，，，，，，，，，，，空，，行怪，\n",
      "batch [630/100] loss 7.042\n",
      "，，，，，，怪，，，，，”怪，，，，，，，，，，，，，，”，，，，，，，，，，，，，，，”怪，，，，，，，，，，僧，，，，行空，，，，父，，，，，，，，，，，，\n",
      "batch [640/100] loss 5.327\n",
      "，，，，，，，，，，，”，，，：“怪，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，”行空，，：“，，\n",
      "batch [650/100] loss 5.098\n",
      "师，，，，，，，，，，”，，，，，，，，，萨，，，那萨，：“，，父，，，僧，，，，，，，，，，，，”说者，：“，，父，，，，怪，，，，，，，，，，，，怪，，，，\n",
      "batch [660/100] loss 4.782\n",
      "，，，，，”悟戒，：“，父河，师，，，，”悟空，，，，戒，；师，，，，，，，，，戒，，，，，，，，，，，，，，，，，，，，，，”悟空，，，悟空，，，“，怪，，，\n",
      "batch [670/100] loss 5.789\n",
      "，，，，父，，，，，怪，，，，，，，尚，，，，，，，，，，者，，，”悟父，：“师，，，，，，，”，，，，，，，，，，，，，，，，，，”，，，，，，，，你，，，，\n",
      "batch [680/100] loss 5.920\n",
      "，，，父，，，，，，，，，，，，，，，，，，，，，，，，，，，悟僧说戒，，，，，，，，，：“你，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "batch [690/100] loss 5.089\n",
      "，，，，，尚，，，，，，，，，，唐戒，，，”，，，，，，，，唐悟沙，：“唐戒，，，，，”唐，，，，，，，，，，：“师戒，，，，，，，唐唐戒，，：“你，父，，，，\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-882022c00f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch [{}/{}] loss {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size= 16\n",
    "time_step= 5\n",
    "LR = 1e-2\n",
    "net = CharRNN(vocab)\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "opt = Adam(net.parameters(), lr= LR)\n",
    "criterion = CrossEntropyLoss()\n",
    "print(net)\n",
    "net.train()\n",
    "hidden = None\n",
    "for i, (x,y) in enumerate(yield_data(batch_size,time_step)):\n",
    "    net.zero_grad()\n",
    "    opt.zero_grad()\n",
    "    input_tensor = Variable(torch.from_numpy(x).float())\n",
    "    target = Variable(torch.from_numpy(y).long())\n",
    "    if use_gpu:\n",
    "        input_tensor = input_tensor.cuda()\n",
    "        target = target.cuda()\n",
    "    output, hidden = net(input_tensor, hidden)\n",
    "    hidden = (hidden[0].data, hidden[1].data)\n",
    "    label = target.contiguous().view(batch_size * time_step)\n",
    "    loss = criterion(output, label.long())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    if i % 10 == 0:\n",
    "        print('batch [{}/{}] loss {:.3f}'.format(i, 100, loss.data))\n",
    "        output_array = output.cpu().detach().numpy().reshape(batch_size * time_step, -1)\n",
    "        ids = np.argmax(output_array, axis=1)\n",
    "        text = ''.join([net.id2char[id_] for id_ in ids])\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9871, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.01\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
